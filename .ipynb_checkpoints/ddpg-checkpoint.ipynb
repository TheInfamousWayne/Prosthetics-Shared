{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from Actor import ActorNetwork\n",
    "from Critic import CriticNetwork\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from ou_noise import OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters:\n",
    "REPLAY_BUFFER_SIZE = 5000\n",
    "REPLAY_START_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.name = 'DDPG' # name for uploading results\n",
    "        self.environment = env\n",
    "        # Randomly initialize actor network and critic network\n",
    "        # along with their target networks\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "\n",
    "        self.actor_network = ActorNetwork(self.sess,self.state_dim,self.action_dim)\n",
    "        self.critic_network = CriticNetwork(self.sess,self.state_dim,self.action_dim)\n",
    "        \n",
    "        # initialize replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "        # Initialize a random process the Ornstein-Uhlenbeck process for action exploration\n",
    "        self.exploration_noise = OUNoise(self.action_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        #print \"train step\",self.time_step\n",
    "        # Sample a random minibatch of N transitions from replay buffer\n",
    "        minibatch = self.replay_buffer.get_batch(BATCH_SIZE)\n",
    "        state_batch = np.asarray([data[0] for data in minibatch])\n",
    "        action_batch = np.asarray([data[1] for data in minibatch])\n",
    "        reward_batch = np.asarray([data[2] for data in minibatch])\n",
    "        next_state_batch = np.asarray([data[3] for data in minibatch])\n",
    "        done_batch = np.asarray([data[4] for data in minibatch])\n",
    "\n",
    "        # for action_dim = 1\n",
    "        action_batch = np.resize(action_batch,[BATCH_SIZE,self.action_dim])\n",
    "\n",
    "        # Calculate y_batch\n",
    "        next_action_batch = self.actor_network.target_actions(next_state_batch)\n",
    "        q_value_batch = self.critic_network.target_q(next_state_batch,next_action_batch)\n",
    "        y_batch = []  \n",
    "        for i in range(len(minibatch)): \n",
    "            if done_batch[i]:\n",
    "                y_batch.append(reward_batch[i])\n",
    "            else :\n",
    "                y_batch.append(reward_batch[i] + GAMMA * q_value_batch[i])\n",
    "        y_batch = np.resize(y_batch,[BATCH_SIZE,1])\n",
    "        # Update critic by minimizing the loss L\n",
    "        self.critic_network.train(y_batch,state_batch,action_batch)\n",
    "\n",
    "        # Update the actor policy using the sampled gradient:\n",
    "        action_batch_for_gradients = self.actor_network.actions(state_batch)\n",
    "        q_gradient_batch = self.critic_network.gradients(state_batch,action_batch_for_gradients)\n",
    "\n",
    "        self.actor_network.train(q_gradient_batch,state_batch)\n",
    "\n",
    "        # Update the target networks\n",
    "        self.actor_network.update_target()\n",
    "        self.critic_network.update_target()\n",
    "\n",
    "        \n",
    "    def noise_action(self,state):\n",
    "        # Select action a_t according to the current policy and exploration noise\n",
    "        action = self.actor_network.action(state)\n",
    "        return action+self.exploration_noise.noise()\n",
    "    \n",
    "    \n",
    "    def action(self,state):\n",
    "        action = self.actor_network.action(state)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def perceive(self,state,action,reward,next_state,done):\n",
    "        # Store transition (s_t,a_t,r_t,s_{t+1}) in replay buffer\n",
    "        self.replay_buffer.add(state,action,reward,next_state,done)\n",
    "\n",
    "        # Store transitions to replay start size then start training\n",
    "        if self.replay_buffer.count() >  REPLAY_START_SIZE:\n",
    "            self.train()\n",
    "\n",
    "        self.time_step = self.critic_network.time_step\n",
    "        if self.time_step % 2000 == 0:\n",
    "            self.actor_network.save_network(self.time_step)\n",
    "            self.critic_network.save_network(self.time_step)\n",
    "\n",
    "        # Re-iniitialize the random process when an episode ends\n",
    "        if done:\n",
    "            self.exploration_noise.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
